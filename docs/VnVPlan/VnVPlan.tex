\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\usepackage{longtable}
\usepackage{float}
\usepackage{enumitem}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}

\input{../Comments}
\input{../Common}

\begin{document}

\title{System Verification and Validation Plan for \progname{}}
\author{\authname}
\date{\today}

\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
Date 1 & 1.0 & Notes\\
Date 2 & 1.1 & Notes\\
\bottomrule
\end{tabularx}

% ~\\
% \wss{The intention of the VnV plan is to increase confidence in the software.
% However, this does not mean listing every verification and validation technique
% that has ever been devised.  The VnV plan should also be a \textbf{feasible}
% plan. Execution of the plan should be possible with the time and team available.
% If the full plan cannot be completed during the time available, it can either be
% modified to ``fake it'', or a better solution is to add a section describing
% what work has been completed and what work is still planned for the future.}

% \wss{The VnV plan is typically started after the requirements stage, but before
% the design stage.  This means that the sections related to unit testing cannot
% initially be completed.  The sections will be filled in after the design stage
% is complete.  the final version of the VnV plan should have all sections filled
% in.}

\newpage

\tableofcontents

\listoftables
\wss{Remove this section if it isn't needed}

\listoffigures
\wss{Remove this section if it isn't needed}

\newpage

\section{Symbols, Abbreviations, and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l}
  \toprule
  \textbf{symbol} & \textbf{description}\\
  \midrule
  T & Test\\
  \bottomrule
\end{tabular}\\

\wss{symbols, abbreviations, or acronyms --- you can simply reference the SRS
  \citep{SRS} tables, if appropriate}

\wss{Remove this section if it isn't needed}

\newpage

\pagenumbering{arabic}

This document ... \wss{provide an introductory blurb and roadmap of the
  Verification and Validation plan}

\section{General Information}

\subsection{Summary}

The following core features of the product will be tested:

\begin{description}
  \item[Custom Form Builder] Allows admins to create and publish survey forms; obtains responses from attendees; and
    allows admins to view responses.
  \item[Registration and Feedback Forms] Allow admins to create or monitor registration forms for events; allow students
    to register for events; generate QR codes and tickets for students to confirm their registrations; receive feedback
    from students regarding events; allow admins to review feedback received from students.
  \item[Attendee Overview] Allow admins to view event attendance; allow admins to update event statuses and inform
    attendees of changes; allow attendees to receive notifications regarding events.
  \item[Backend Analytics] Allow admins to query any set of form questions and view aggregated results from the query;
    allow admins to freely visualize relevant data points from surveys; allow admins to export analytics data as
    SCV/Excel reports.
\end{description}

\noindent The testing ranges from individual unit testing to overall system integration testing to ensure that all functionality
works as intended.
\subsection{Objectives}

The purpose of this verification and validation plan is to evaluate the reliability, accuracy, and conformity of the
\teamname~with its requirements. The main objectives are to build confidence in the correctness of the overall system,
verify that all core features and functionality operate as intended, and demonstrate that the system will perform well
when deployed by MES. The testing focus will be on ensuring functionality, performance, reliability, and correctness of
the core components, both in isolation and when integrated with the rest of the system. These core components are
essential to the completion of the capstone, so they must function correctly in order to meet the project goals.

User Interface and application usability are out of scope for the testing roadmap outlined in this plan as they are
addressed in the project extras. Also out of scope for this plan is verification of external libraries and third-party
tools (such as PostgreSQL and React) as it is assumed that these products have been validated by their respective
developers. This plan prioritizes the testing and verification of the team's bespoke components and their integration
with the overall system.

\subsection{Extras}

\subsubsection{Usability Report}
The usability report will evaluate the overall ease-of-use of the application for both admins and attendees. This
includes definition of evaluation metrics, description of the evaluation process, evaluation results, and an analysis of
the data gathered from the evaluation to determine the usability of the application based on its conformity to the
outlined metrics.

This extra is relevant to the project because the final product is an application which the client
intends to distribute to the public and expects them to use it for every event.

\subsubsection{Wireframe Report}
The wireframe report will document the design process for the application user interface for both admins and attendees.
It will justify design decisions and how the team expects these visual representations to allow users to navigate the
application with a reasonably low amount of effort.

All users will interact regularly with the system through an application user interface; design principles employed to
make these interactions productive are therefore relevant to the project.

\subsection{Relevant Documentation}
The following documents are relevant to the context of the verification and validation plan:
\begin{enumerate}[align=left,
  leftmargin=*,
  labelsep=1em,
  itemindent=0em, font=\bfseries]
  \item {\bfseries SRS:} Defines all functional and non-functional requirements which form the basis of the verification
    process \citep{SRS}.
  \item {\bfseries Development Plan:} Outlines the code and development structure as well as lists third-party
    technologies which will be used for development, and defines the technology to be used for testing \citep{DVP}.
  \item {\bfseries Usability Report:} Provides a thorough evaluation of the application and user interface for design
    validation.
  \item {\bfseries Wireframe Report:} Documents the user interface design process, including mockups from which design
    evaluations may stem.
\end{enumerate}

\section{Plan}

This sections provides a detailed overview of the validation and verification of the system, including specific validation and verification roles, requirements verification, and design verification.

\subsection{Verification and Validation Team}

\wss{Your teammates.  Maybe your supervisor.
  You should do more than list names.  You should say what each person's role is
  for the project's verification.  A table is a good way to summarize this information.}
\begin{itemize}
  \item \textbf{Ibrahim Quraishi}: Backend Tester. Verifies that the backend service logic complies with all related functional requirements as well as nonfunctional requirements such as performance and maintainability requirements.
  \item \textbf{Omar Al-Asfar}: Mobile Application Tester. Verifies the mobile application complies with all related functional requirements as well as nonfunctional requirements such as usability and look and feel requirements.
  \item \textbf{Rayyan Suhail}: Web Application Tester. Verifies the admin web application complies with all related functional requirements as well as nonfunctional requirements such as usability and look and feel requirements. 
  \item \textbf{Virochaan Ravichandran Gowri}: Integration Tester. Tests the overall integration of system components and their compliance with non-functional requirements such as performance and maintainability.
  \item \textbf{Mohammad Mahdi Mahboob}: Security Tester. Tests the security of the system by checking for compliance with security standards and legal requirements.
  \item \textbf{Luke Schermann}: Supervisor and Acceptance Tester. Supervises the overal implementation of the system, ensuring stakeholder requirements are met. Responsible for attending acceptance testing workshops with the developers for the admin web portal.
\end{itemize}

\subsection{SRS Verification}

\wss{List any approaches you intend to use for SRS verification.  This may
  include ad hoc feedback from reviewers, like your classmates (like your
  primary reviewer), or you may plan for something more rigorous/systematic.}

\wss{If you have a supervisor for the project, you shouldn't just say they will
read over the SRS.  You should explain your structured approach to the review.
Will you have a meeting?  What will you present?  What questions will you ask?
Will you give them instructions for a task-based inspection?  Will you use your
issue tracker?}

\subsubsection{Requirements Validation}
The first step of verification for the software requirements specification is to design and implement a suite of tests that provide a comprehensive coverage of the functional and non functional requirements. To ensure traceability between test cases and requirements, each test case should maintain a clear documentation referencing which requirements it covers, its inputs, and expected behaviour. The requirements validation will be split into two parts. 

\textbf{Component Level}
Each of the main components of the system listed below will first be tested in isolation without connections to other components. Wherever necessary, dummy data will be used to fill in inputs or outputs from other system components.
\begin{itemize}
  \item \textbf{Web Admin Application:} Testing of UI elements, event and survey creation, form module creation, and analytics viewing. May use a combination of React Testing Library and Jest.
  \item \textbf{Mobile Attendee Application:} Testing of UI elements, event registration, survey completion, response editing. May use a combination of React Testing Library and Jest.
  \item \textbf{Backend Server:} Testing of data, security and encryption, server capacity limits, and API responses. This will also include testing and performance measurements of database querying and posting. Testing framework(s) will depend on language used to build the backend. 
\end{itemize}

\textbf{System Level}
  After integration of each of the system's components are complete, additional system level testing will be done to verify end to end behaviour. The system will be deployed to a staging environment to carry out automated testing using a framework such as Selenium/Appium as well as manual testing from the frontend sides by developers. This will include testing of non functional requirements which can only be done on the entire system such as speed, latency, and capacity requirements.

\subsubsection{Stakeholder Review}
  While regular weekly update meetings and/or feature demonstrations will occur with our supervisor, Luke Schermann, an additional formal stakeholder review will be conducted with Luke and any other MES executives who are interested in attending. Assuming the supervisor has read through the SRS, a comprehensive walkthrough of the entire system will be presented, including a summary of key requirements and conduct end to end demonstrations of system workflows with a focus on the administrative user perspective. The purpose of the review is to confirm that the system meets all requirements specified by the stakeholders and provide an opportunity to gather feedback on the completed system to address any outstanding concerns before the final deployment. A list of specific feedback questions will be prepared for the meeting, such as questions on project scoping, additional constraints, and whether project goals have been met.
  
  \subsubsection{User Acceptance Testing}
  User acceptance testing will be conducted on a staging deployment of the system with a group of about 10 test users. Each user will be given a list of actions to perform on the attendee mobile application without help from developers, such as looking up and registering for an evert or filling out a survey. The user will then provide feedback on the experience of completing these actions which will be analyzed for final improvments before the initial launch.

\subsubsection{Pilot Launch}
  By request of the project supervisor, a pilot program of the system will be launched in January. This pilot program will involve testing the system in a scenario with real users, such as the Fireball Formal and CFES National Survey. The system will be deployed to a production environment, where users will use the application as intended. Feedback will be collected by attendees and admins as well as feedback from the supervisor on user satisfaction. to analyze and address any final problems before the official launch of the system.

\begin{longtable}{|p{4cm}|p{11cm}|}
\hline
\textbf{Review Category} & \textbf{Checklist Items} \\
\hline
\endfirsthead

\hline
\textbf{Review Category} & \textbf{Checklist Items} \\
\hline
\endhead

\textbf{Requirements Validation} &
\begin{itemize}
  \item Verify all functional and non-functional requirements are covered
  \item Confirm each test case links to a requirement, inputs, and results
  \item Ensure dummy data and isolated component tests are realistic
  \item Confirm integration tests demonstrate expected end-to-end data flow
  \item Review performance and capacity testing coverage
\end{itemize}
\\
\hline

\textbf{Stakeholder Review} &
\begin{itemize}
  \item Schedule formal review with supervisor
  \item Send SRS document prior to review
  \item Summarize key requirements and implemented features
  \item Present full system walkthrough
  \item Prepare review questions on scope and alignment
  \item Record and log stakeholder feedback and actions on GitHub Issues
  \item Execute feedback actions
\end{itemize}
\\
\hline

\textbf{User Acceptance Testing} &
\begin{itemize}
  \item Deploy staging environment for test users
  \item Provide structured task list (event registration, surveys, etc.)
  \item Collect and analyze usability feedback as GitHub Issues
  \item Execute feedback actions
\end{itemize}
\\
\hline

\textbf{Pilot Launch} &
\begin{itemize}
  \item Prepare production deployment for pilot event
  \item Conduct testing with actual users
  \item Monitor stability and data integrity
  \item Collect feedback from admins, attendees, and supervisor
  \item Document and prioritize issues found on GitHub Issues
  \item Execute feedback actions
  \item Final confirmation of readiness for official launch
\end{itemize}
\\
\hline

\caption{SRS Verification and Review Checklist}
\label{tab:srs_verification_checklist}
\end{longtable}


\subsection{Design Verification}

The design verification of the system will involve verifying that the proposed system architecture and detailed design satisfy all requirements outlined in the SRS. Design verifications will be conducted by the supervisor, Luke Schermann, as well as through peer reviews. The process will ensuring the design's goals are achievable within the given time frame and that there are no inconsistencies or conflicts between design decision.

For peer reviews, design documents such as architecture diagrams, data flow diagrams, and entity-relationship models will be reviewed for feasibility, unambiguity, clarity, and  consistency with requirements specified in the SRS. Reviewers will also have the chance to provide feedback on the design to identify potential design flaws. Peer feedback will be tracked using GitHub Issues and implemented before the formal design review with the supervisor.

For the formal design review, the supervisor will be presented with a detailed walkthrough of the system architecture and rationale for key design decisions. This presentation will cover the entire system architecure with an emphasis on the admin perspective of system. The supervisor will provide feedback and propose changes to ensure the design aligns with the interest of major stakeholders.

\begin{longtable}{|p{4cm}|p{11cm}|}
\hline
\textbf{Review Category} & \textbf{Checklist Items} \\
\hline
\endfirsthead

\hline
\textbf{Review Category} & \textbf{Checklist Items} \\
\hline
\endhead

\textbf{Peer Design Review} &
\begin{itemize}
  \item Review architecture diagrams for accuracy, feasibility, and consistency with SRS requirements
  \item Check data flow diagrams for completeness and clarity
  \item Validate entity-relationship models against expected database requirements
  \item Confirm that design elements are unambiguous and understandable to all team members
  \item Identify potential design flaws or inefficiencies and propose alternatives
  \item Verify that design decisions align with project goals and constraints
  \item Record all peer feedback in GitHub Issues
  \item Execute feedback actions and update design documents
\end{itemize}
\\
\hline

\textbf{Supervisor Design Review} &
\begin{itemize}
  \item Prepare a presentation covering system architecture, key design decisions, and rationale
  \item Gather feedback on alignment with stakeholder needs and SRS requirements
  \item Discuss any identified risks or assumptions and how they will be mitigated
  \item Document supervisor feedback and proposed design changes with GitHub Issues
  \item Execute feedback actions and update design documentation accordingly
\end{itemize}
\\
\hline

\caption{Design Verification and Review Checklist}
\label{tab:design_verification_checklist}
\end{longtable}


\subsection{Verification and Validation Plan Verification}

To verify the verification and validation strategies presented in this document, a series of peer reviews and internal reviews will be done. Before peer reviews, developers will review the completed document by reviewing sections written by other developers and compare the content with their own contributions to the document. This will ensure there are no inconsistencies between sections such as inconstent language or conflicting test case definitions. The peer review will be conducted by members of another team, who will conduct a review given a coverage checklist derived from the requirements in SRS to ensure the proposed plan provides complete coverage of all outlined requirements.
\\
Addionally, several mutation tests must be done for every system test and unit test outlined in this document to ensure robustness of the proposed test suite on each of the systems components.

\begin{longtable}{|p{4cm}|p{11cm}|}
\hline
\textbf{Review Category} & \textbf{Checklist Items} \\
\hline
\endfirsthead

\hline
\textbf{Review Category} & \textbf{Checklist Items} \\
\hline
\endhead

\textbf{Internal Developer Review} &
\begin{itemize}
  \item Cross-check sections written by other developers for consistency
  \item Verify that all test case definitions align with corresponding requirements
  \item Ensure there are no conflicting assumptions between sections
\end{itemize}
\\
\hline

\textbf{Peer Team Review} &
\begin{itemize}
  \item Create a coverage checklist derived from SRS requirements to confirm all requirements are tested
  \item Identify any missing or ambiguous verification strategies
  \item Provide feedback to improve clarity and completeness of the plan on GitHub Issues
  \item Perform feedback actions
\end{itemize}
\\
\hline

\textbf{Mutation Testing Validation} &
\begin{itemize}
  \item Conduct mutation tests for all unit and system tests to evaluate robustness
  \item Ensure that all mutations are detected or properly justified if surviving
  \item Document weak or incomplete test cases as GitHub Issues
  \item Update unit and system test specification
\end{itemize}
\\
\hline

\caption{Verification and Validation Plan Review Checklist}
\label{tab:vnv_plan_checklist}
\end{longtable}


\subsection{Implementation Verification}

To make sure that the system works as intended and meets all the goals set out in the SRS, our team will verify the implementation through both testing and review processes. The focus will mainly be on the four main parts: the \textbf{Custom Form Builder}, \textbf{Registration and Feedback Forms}, \textbf{Attendee Overview}, and \textbf{Backend Analytics}.

\subsubsection*{Dynamic Verification}
Dynamic verification will confirm that each feature works correctly through practical testing during development.

\begin{itemize}
  \item \textbf{Unit Testing:} Every module will be tested on its own using Jest and the React Testing Library. These tests will check things like whether form fields are created properly, if branching logic behaves as expected, and whether data is saved correctly in the database. 
  \item \textbf{Integration Testing:} Once the individual parts are stable, integration tests will be done to make sure the frontend and backend communicate properly. These tests will confirm that data moves smoothly between the admin portal, the backend server, and the shared database used by the other capstone teams.
  \item \textbf{System and Acceptance Testing:} The final round of testing will simulate real MES events, such as the Fireball Formal or CALE Conference. This will test the full flow—from creating a form, registering attendees, collecting feedback, and generating analytics. We will also check that pages load quickly (forms within about two seconds and analytics within about four) and that all security and accessibility requirements are met. 
\end{itemize}

\subsubsection*{Static Verification}
\begin{itemize}
  \item \textbf{Code Reviews and Walkthroughs:} Every major change will go through a peer review before being merged. The team will check that the code and the logic matches what’s described in the SRS.
  \item \textbf{Static Analysis Tools:} Automatic tools like ESLint and Prettier will be used through GitHub Actions to keep formatting consistent and catch syntax or logic mistakes. We’ll also run scans to identify any issues in third-party libraries (for example, \texttt{npm audit}).
\end{itemize}

\subsubsection*{Usability Verification}
As part of the final CAS 741 presentation, we will also conduct a short usability check. MES members and students will try out basic workflows such as creating a form, signing up for an event, or checking analytics. Their feedback on how easy or confusing it feels will help us fine-tune the interface before final deployment.

\subsection{Automated Testing and Verification Tools}

\begin{itemize}
    \item \textbf{Vitest:} Used as the primary unit and integration testing framework for the frontend. It integrates seamlessly with the Vite development environment and allows for efficient automated testing of components, hooks, and application logic. Vitest supports mocking and snapshot testing, which ensures that UI and logic remain consistent after code changes.

    \item \textbf{Playwright:} Utilized for automated end-to-end testing to simulate user interactions across different browsers and devices. This tool can be used to validate complete user workflows such as event registration and form creation and submissions. This will help ensure system functionality and reliability from the user's perspective.

    \item \textbf{Postman and Newman:} Employed for API-level testing and verification. Postman allows the team to design and execute test cases for RESTful endpoints, while Newman enables command-line execution of these tests within the CI/CD pipeline. Together, they ensure backend consistency and correct response handling.

    \item \textbf{GitHub Actions:} Serves as the continuous integration (CI) platform that automates testing and verification tasks. It will execute unit tests, end-to-end tests, and linters automatically on each push or pull request. This will ensure that all commits meet quality and functionality and don't break the system before merging.

    \item \textbf{pgTAP:} Used for database testing in PostgreSQL. It can help ensure database integrity and ensure that that database schema and back-end logic is implemented correctly through unit testing.
\end{itemize}

\subsection{Software Validation}

\subsubsection*{Stakeholder Review and Demonstrations}
Validation will occur continuously through review sessions with the project supervisor, Luke Schuurman. Each major milestone will be demonstrated to him for feedback to ensure that the system aligns with MES needs. The \textbf{Rev 0 demonstration} will serve as the first major validation checkpoint, where core workflows—form creation, registration, feedback submission, and analytics—will be shown to the supervisor for approval.

After the scheduled Rev 0 demo, the team will conduct a dedicated review meeting with the supervisor to gather specific feedback on usability, workflow logic, and data visualization accuracy. This feedback will guide refinements for later iterations and integration with other teams’ modules.

\subsubsection*{Task-Based User Testing}
Toward the end of the development cycle, we plan to conduct task-based inspections where selected MES members and student users will complete typical actions such as:
\begin{itemize}
  \item Creating a registration or feedback form
  \item Submitting a form as an attendee
  \item Viewing analytics in the admin dashboard
\end{itemize}
Their success rate and feedback will help validate that the system is intuitive and easy to use without any formal training.

\subsubsection*{External Data for Validation}
There are no external data sets required for validation since this is a new system replacing manual workflows. However, data collected from real events (e.g., Fireball Formal or CALE Conference) will later be compared with historical Google Form data to confirm that the new system provides the same or improved accuracy and completeness of records.

\section{System Tests}

This section outlines the the system wide tests to validate both functional and non-functional requirements defined in our Software Requirements Specification.

\subsection{Tests for Functional Requirements}

This section verifies that the implemented system fulfills all \textbf{Functional Requirements (FR-1 through FR-11)} defined in the SRS. 
The tests are grouped into three \textbf{Areas of Testing}: \textit{Admin Portal (AP)}, \textit{Attendee Application (AA)}, and \textit{System Integration and Security (SI)}. 
\subsubsection{Area of Testing 1 – Admin Portal}

\begin{enumerate}

\item[\textbf{Test-FR-AP-1}] \textbf{Create Form}\\
\textbf{Type:} Manual\\
\textbf{Initial State:} Admin is logged in with form creation privileges.\\
\textbf{Input/Condition:} Admin enters a new form name and adds multiple field types (text, checkbox, rating).\\
\textbf{Expected Output:} The new form module is created and is visible in the database and in the admin dashboard.\\
\textbf{Test Case Derivation:} This test verifies the system’s ability to support dynamic form creation and storage as required by FR-1 and FR-7.\\
\textbf{How the test will be performed:} The tester will create a new form and confirm it appears correctly in the interface and database.\\[6pt]

\item[\textbf{Test-FR-AP-2}] \textbf{View and Analyze Form Data}\\
\textbf{Type:} Manual\\
\textbf{Initial State:} At least two forms with collected responses exist.\\
\textbf{Input/Condition:} Admin selects multiple forms and clicks button to view report analytics\\
\textbf{Expected Output:} Aggregated analytics are displayed correctly and match backend data.\\
\textbf{Test Case Derivation:} This test confirms data organization and analytical visualization functionalities described in FR-2 and FR-8, ensuring accurate reporting and analytics.\\
\textbf{How the test will be performed:} The tester will generate combined analytics and confirm that the displayed charts match backend results.\\[6pt]

\item[\textbf{Test-FR-AP-3}] \textbf{Manage Events via Dashboard}\\
\textbf{Type:} Manual\\
\textbf{Initial State:} Several events with participant data exist.\\
\textbf{Input/Condition:} Admin accesses the dashboard and applies filters (e.g., pending payment).\\
\textbf{Expected Output:} Filtered results display accurately with correct status counts.\\
\textbf{Test Case Derivation:} This test validates event management, filtering, and data persistence mechanisms described in FR-5 and FR-6.\\
\textbf{How the test will be performed:} The tester will apply filters and verify that displayed data matches expected event records.\\[6pt]

\item[\textbf{Test-FR-AP-4}] \textbf{View and Export Event Analytics}\\
\textbf{Type:} Manual\\
\textbf{Initial State:} Events and surveys contain collected feedback.\\
\textbf{Input/Condition:} Admin selects an event and exports analytics.\\
\textbf{Expected Output:} Correct analytics display and downloadable report matches system data.\\
\textbf{Test Case Derivation:} This test ensures that analytics generation and report export satisfy the requirements for event statistics and reporting defined in FR-8 and FR-10.\\
\textbf{How the test will be performed:} The tester will view analytics for an event, export the report, and compare the output to dashboard data.\\[12pt]

\end{enumerate}

\subsubsection{Area of Testing 2 – Attendee Application}

\begin{enumerate}

\item[\textbf{Test-FR-AA-1}] \textbf{Register for Event}\\
\textbf{Type:} Automatic\\
\textbf{Initial State:} An event is open and attendee is authenticated.\\
\textbf{Input/Condition:} Attendee submits an event registration form.\\
\textbf{Expected Output:} Registration is saved and a confirmation message is displayed and sent to attendee and a QR ticket is generated.\\
\textbf{Test Case Derivation:} This test validates the registration workflow and QR ticket generation process as required by FR-3, FR-4, and FR-6.\\
\textbf{How the test will be performed:} An automated test will submit registration data and verify the confirmation message and QR code has been created. Will confirm that the event data has also been updated in the database.\\[6pt]

\item[\textbf{Test-FR-AA-2}] \textbf{Access Event Listings}\\
\textbf{Type:} Manual\\
\textbf{Initial State:} The system has multiple upcoming and past events.\\
\textbf{Input/Condition:} Attendee views ``Upcoming'' and ``Registered Events'' tabs.\\
\textbf{Expected Output:} Each tab displays correct events for that category.\\
\textbf{Test Case Derivation:} This test confirms accurate retrieval and filtering of events for authenticated users as required by FR-3 and ensures user can view event info.\\
\textbf{How the test will be performed:} The tester will view event lists and verify they match stored registration records. Tester will also ensure event info shows up correctly and matches database.\\[6pt]

\item[\textbf{Test-FR-AA-3}] \textbf{Fill out Form}\\
\textbf{Type:} Automatic\\
\textbf{Initial State:} A survey is available and attendee is authenticated.\\
\textbf{Input/Condition:} Attendee completes and submits the survey.\\
\textbf{Expected Output:} Survey responses are stored and a confirmation message appears.\\
\textbf{Test Case Derivation:} This test ensures that survey submission and feedback processing meet the specifications in FR-7, FR-9.\\
\textbf{How the test will be performed:} An automated test will submit survey responses and confirm that results are stored and submission is found in the centralized database.\\[12pt]

\end{enumerate}

\subsubsection{Area of Testing 3 – System Integration and Security}

\begin{enumerate}

\item[\textbf{Test-FR-SI-1}] \textbf{Real-Time Data Synchronization}\\
\textbf{Type:} Automatic\\
\textbf{Initial State:} Admin and attendee interfaces are active.\\
\textbf{Input/Condition:} Admin updates event details (e.g., venue or date).\\
\textbf{Expected Output:} Attendee view updates automatically without manual refresh.\\
\textbf{Test Case Derivation:} This test validates the real-time synchronization and data propagation features required by FR-6 and FR-7 and ensures that both databases are connected to the interface.\\
\textbf{How the test will be performed:} An automated test will modify event data and verify that the attendee view reflects the change immediately.\\[6pt]

\item[\textbf{Test-FR-SI-2}] \textbf{Role-Based Access Verification}\\
\textbf{Type:} Manual\\
\textbf{Initial State:} Admin and attendee accounts exist with distinct permissions.\\
\textbf{Input/Condition:} Each user attempts to access admin-only features.\\
\textbf{Expected Output:} Admin gains access but attendee is denied access with proper error handling.\\
\textbf{Test Case Derivation:} This test checks role-based access control and authorization mechanisms as defined in FR-11 ensuring only specific users have access to admin controls.\\
\textbf{How the test will be performed:} The tester will log in as both user types and attempt to access admin features to confirm proper restriction.\\[12pt]

\end{enumerate}

% \subsection{Functional Requirement Coverage Summary}




\subsection{Tests for Nonfunctional Requirements}

\wss{The nonfunctional requirements for accuracy will likely just reference the
  appropriate functional tests from above.  The test cases should mention
  reporting the relative error for these tests.  Not all projects will
  necessarily have nonfunctional requirements related to accuracy.}

\wss{For some nonfunctional tests, you won't be setting a target threshold for
passing the test, but rather describing the experiment you will do to measure
the quality for different inputs.  For instance, you could measure speed versus
the problem size.  The output of the test isn't pass/fail, but rather a summary
table or graph.}

\wss{Tests related to usability could include conducting a usability test and
  survey.  The survey will be in the Appendix.}

\wss{Static tests, review, inspections, and walkthroughs, will not follow the
format for the tests given below.}

\wss{If you introduce static tests in your plan, you need to provide details.
How will they be done?  In cases like code (or document) walkthroughs, who will
be involved? Be specific.}

\subsubsection{Area of Testing1}

\paragraph{Title for Test}

\begin{enumerate}

\item{test-id1\\}

Type: Functional, Dynamic, Manual, Static etc.

Initial State:

Input/Condition:

Output/Result:

How test will be performed:

\item{test-id2\\}

Type: Functional, Dynamic, Manual, Static etc.

Initial State:

Input:

Output:

How test will be performed:

\end{enumerate}

\subsubsection{Area of Testing2}

...

\subsection{Traceability Between Test Cases and Requirements}

\begin{table}[H]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Test ID} & \textbf{Functional Requirements Covered} \\
\hline
Test-FR-AP-1 & FR-1, FR-7 \\
Test-FR-AP-2 & FR-2, FR-8 \\
Test-FR-AP-3 & FR-5, FR-6 \\
Test-FR-AP-4 & FR-8, FR-10 \\
Test-FR-AA-1 & FR-3, FR-4, FR-6 \\
Test-FR-AA-2 & FR-3\\
Test-FR-AA-3 & FR-7, FR-9\\
Test-FR-SI-1 & FR-6, FR-7 \\
Test-FR-SI-2 & FR-11 \\
\hline
\end{tabular}
\caption{Mapping of System Tests to Functional Requirements}
\end{table}

\section{Unit Test Description}

\wss{This section should not be filled in until after the MIS (detailed design
  document) has been completed.}

\wss{Reference your MIS (detailed design document) and explain your overall
philosophy for test case selection.}

\wss{To save space and time, it may be an option to provide less detail in this section.
For the unit tests you can potentially layout your testing strategy here.  That is, you
can explain how tests will be selected for each module.  For instance, your test building
approach could be test cases for each access program, including one test for normal behaviour
and as many tests as needed for edge cases.  Rather than create the details of the input
and output here, you could point to the unit testing code.  For this to work, you code
needs to be well-documented, with meaningful names for all of the tests.}

\subsection{Unit Testing Scope}

\wss{What modules are outside of the scope.  If there are modules that are
  developed by someone else, then you would say here if you aren't planning on
  verifying them.  There may also be modules that are part of your software, but
  have a lower priority for verification than others.  If this is the case,
  explain your rationale for the ranking of module importance.}

\subsection{Tests for Functional Requirements}

\wss{Most of the verification will be through automated unit testing.  If
  appropriate specific modules can be verified by a non-testing based
  technique.  That can also be documented in this section.}

\subsubsection{Module 1}

\wss{Include a blurb here to explain why the subsections below cover the module.
  References to the MIS would be good.  You will want tests from a black box
  perspective and from a white box perspective.  Explain to the reader how the
  tests were selected.}

\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}

Initial State:

Input:

Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed:

\item{test-id2\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}

Initial State:

Input:

Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed:

\item{...\\}

\end{enumerate}

\subsubsection{Module 2}

...

\subsection{Tests for Nonfunctional Requirements}

\wss{If there is a module that needs to be independently assessed for
  performance, those test cases can go here.  In some projects, planning for
  nonfunctional tests of units will not be that relevant.}

\wss{These tests may involve collecting performance data from previously
  mentioned functional tests.}

\subsubsection{Module ?}

\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}

Initial State:

Input/Condition:

Output/Result:

How test will be performed:

\item{test-id2\\}

Type: Functional, Dynamic, Manual, Static etc.

Initial State:

Input:

Output:

How test will be performed:

\end{enumerate}

\subsubsection{Module ?}

...

\subsection{Traceability Between Test Cases and Modules}

\wss{Provide evidence that all of the modules have been considered.}

\bibliographystyle{plainnat}

\bibliography{../../refs/References}

\newpage

\section{Appendix}

This is where you can place additional information.

\subsection{Symbolic Parameters}

The definition of the test cases will call for SYMBOLIC\_CONSTANTS.
Their values are defined in this section for easy maintenance.

\subsection{Usability Survey Questions?}

\wss{This is a section that would be appropriate for some projects.}

\newpage{}

\section*{Appendix --- Reflection}

The information in this section will be used to evaluate the team members on the
graduate attribute of Lifelong Learning.

% \input{../Reflection.tex}

\subsubsection*{Group Reflection}
\begin{enumerate}
 
  \item \textbf{What knowledge and skills will the team collectively need to acquire to successfully complete the verification and validation of your project?
  Examples of possible knowledge and skills include dynamic testing knowledge, static testing knowledge, specific tool usage, Valgrind etc.  You should look to identify at least one item for each team member.}
  \begin {enumerate}
    \item \textbf{Automated Testing with CI/CD}: We need to ensure we understand how test pipelines work and how some of these automated testing tools such as ViteTest and Playwright will integrate within our workflows. As mentioned in the SRS we also need to build upon our knowledge on GitHub Actions to ensure that we utilize it properly to have a proper testing workflow.
    \item \textbf{Static Testing and Code Quality Assurance}: Understand and implement static testing methods such as code inspections and code reviews. It will help us find more tools to help perform static analysis to find defects early in the development lifecycle.
    \item \textbf{Dynamic Testing}: Build upon pre-existing dynamic testing skills and find ways to use it in our use case. This includes developing better system and unit testing skills and developing skills in creating test cases.
    \item \textbf{Usability Testing}: Though this might relate more to the Usability report, we need to develop this skill to ensure that we can create a positive user experience. 
    \item \textbf{ViteTest and Playwright}: These two tools are the most important tools that we will be using throughout our testing phase of the project so it is important we gain a good understanding of these tools to utilize them to the highest level.
  \end{enumerate}
  \item \textbf{For each of the knowledge areas and skills identified in the previous
  question, what are at least two approaches to acquiring the knowledge or
  mastering the skill?  Of the identified approaches, which will each team
  member pursue, and why did they make this choice?}
  \begin{enumerate}
    \item \textbf{Automated Testing with CI/CD}: 
    To acquire this knowledge we can study the official documentation and tutorials for GitHub Actions as well as related automated testing tools. This will help us understand configuration and workflow syntax. We can also analyze open-source repositories that employ CI/CD with similar front-end stacks to observe best practices and reusable workflow patterns. 
    \item \textbf{Static Testing and Code Quality Assurance}: 
    This knowledge can be developed by completing structured online modules or guides on static code analysis, focusing on tools that are easy to use for our use case. We can also conduct regular peer-led code reviews and checklist-based inspections to ensure we hold each other accountable and figure out our code patterns. 

    \item \textbf{Dynamic Testing}: 
    The team can strengthen this skill by taking short online courses or tutorials on unit, integration, and system testing methodologies and find videos on testcase design. We can also look back to our SFWRENG 3S03 where we worked first hand on developing test cases to ensure we can find ways to maximize code coverage.

    \item \textbf{Usability Testing}: 
    To develop this skill, the team could review Human–Computer Interaction (HCI) principles and usability evaluation methods through our course work. We can also look into tutorials online about effective UX design to and link usability testing to front-end design. We can also discuss with our stakeholders and supervisors their expectations to develop our own testing framework.


    \item \textbf{ViteTest and Playwright}: 
    The team can gain proficiency by following official tutorials and documentation to build example test suites and understand configuration options for both tools. We can also look to open-source projects or community-driven examples to learn advanced testing features that these tools offer. \newline
\end{enumerate}

\textbf{What each member will pursue:}
\begin{enumerate}
    \item \textbf{Virochaan}:  Virochaan will focus on mastering \textbf{Automated Testing with CI/CD}, particularly the integration of GitHub Actions with Vitest and Playwright. Since he was specified as the DevOps manager it will be important to know to create proper Github workflows. From this he will also need to know the details of \textbf{Vitest and Playwright} to ensure that we develop the proper testing protocol for this. He will primarily focus on looking at tutorials and find open source projects with workflows that we can look to understand to develop our own.

    \item \textbf{Mohammad}: Mohammad has worked with static coding tools in the past so he will look to develop those skills further by focusing on \textbf{Static Testing}. He will do this by researching static testing tools for our usecase and looking at tutorials and examples where static testing has been done for full-stack apps.

    \item \textbf{Omar}: Omar has experience developing mobile apps so he will focus on \textbf{Usability Testing} to ensure that he can develop the best user experience possible. He will do this by primarily looking at video tutorials on usability testing and in our supervisor meetings develop a testing framework for our UI.

    \item \textbf{Ibrahim}: Ibrahim will look to focus on \textbf{Dynamic Testing} with \textbf{Vitest and Playwright}. These tools also offer dynamic testing capabilities and it is important we figure out how to connect the two effectively. Ibrahim will focus on looking through tutorials for the two technologies and watching videos on testing methodologies so we can implement the methodologies for our usecase.

    \item \textbf{Rayyan}: Rayyan has experience working with testing through his coop experiences. To build upon this he will focus on \textbf{Dynamic Testing} and \textbf{Static Testing} and will look to be well rounded on this. In our roles we also identified him as a web app tester so this will allow him to develop good testcases to accomplish this. He will primarily focus on looking at tutorials on dynamic and static testing and how we can connect the two together to make a better testing framework.

\end{enumerate}
\end{enumerate}

\subsubsection*{Mohammad Mahdi Mahboob}
\begin{enumerate}[align=left,
  leftmargin=*,
  labelsep=1em,
  itemindent=0em, font=\bfseries]
  \item Writing the summary listing the core features which will be tested as well as the sections detailing the extras
    was simple. The core components had already been identified and articulated in the SRS and clarified with the
    client, so we know exactly what they entail and have a good idea of how they interact. The extras were already
    defined in the Problem Statement and Goals document, and their relevance to the project were also justified in that
    document as well. The points were reiterated here.
  \item Defining the objectives and the scope of the project was a bit difficult. There are many considerations to be
    made when developing the product and application, and as a result there are an exhaustive list of items that require
    validation. We were able to clarify with the TA what ought to be mentioned in the VnV plan, and based on that, I was
    able to narrow down the focus of the testing.
\end{enumerate}
\subsubsection*{Virochaan Ravichandran Gowri Reflection}
\begin{enumerate}
  \item \textbf{What went well while writing this deliverable?} \\
  Since we had already completed the SRS deriving test cases to ensure that the different requirements were covered was quite easy and straightforward. We used some of the diagrams and components identified within the SRS to breakdown the functional system tests and for the NFR system tests 
  \item \textbf{What pain points did you experience during this deliverable, and how did you resolve them?} \\
  One pain point that was more of an annoyance was ensuring traceability and that we kept a good track of what test cases were covering which requirements and not having too much overlap on the requirements we were validating. We also wanted to ensure that the test were actually feasible and did not put too high of a burden on us. This meant that we had to consider both manual and automatic tests that could be accomplished. 
\end{enumerate}
\subsubsection*{Rayyan Suhail Reflection}
\begin{enumerate}
\item \textbf{What went well while writing this deliverable?} \
Documenting the Implementation Verification and Software Validation sections was straightforward since these parts aligned closely with our testing plan. I already had a clear picture of how unit, integration, and system tests would work with tools like Jest, React Testing Library, and ESLint, so outlining the steps was simple.

\item \textbf{What pain points did you experience during this deliverable, and how did you resolve them?} \
The main challenge was keeping the content concise and focused instead of diving too deep into technical details. I resolved this by refining the language and aligning the content with the SRS to avoid duplication or overlap with other sections.
\end{enumerate}
\end{document}
